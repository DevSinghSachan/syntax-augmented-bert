{
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 50265,
  "model_type": "bert_baseline",
  "crf": true,
  "use_syntax": false,
  "syntax": {
      "syntax_encoder": "GATEncoder",
      "embed_position": false,
      "use_dep_rel": false,
      "use_subj_obj": false,
      "prune_k": -1,
      "finetune_bert": true,
      "hidden_size": 1024,
      "emb_size": 1024,
      "mlp_layers": 1,
      "pooling": "max",
      "contextual_rnn": false,
      "rnn_hidden": 512,
      "rnn_layers": 1,
      "num_layers": 4,
      "rnn_dropout": 0.05,
      "layer_dropout": 0.5,
      "input_dropout": 0.5,
      "adj_self_loop": true,
      "gelu_dropout": 0.1,
      "layer_prepostprocess_dropout": 0.1,
      "late_fusion_gated_connection": true,
      "late_fusion_gate": "HighwayGateLayer",
      "tf_enc_gated_connection": false,
      "tf_enc_gate": "HighwayGateLayer",
      "tf_enc_use_ffn": true,
      "tf_enc_act_at_skip_connection": false,
      "pre_layer_norm": true,
      "post_layer_norm": false
  },
  "optimizer": {
      "optim": "adam",
      "learning_rate": 2e-5,
      "weight_decay": 0.00,
      "lr_scheduler": "warmup_linear",
      "warmup_steps": 0,
      "freeze_steps": 0,
      "parameter_groups": [
          [["classifier"], {"weight_decay": 0.0, "optim": "adam", "learning_rate": 2e-5, "lr_scheduler": "warmup_linear", "freeze_steps": 0, "warmup_steps": 0}],
          [["syntax_encoder"], {"weight_decay": 0.0, "optim": "adam", "learning_rate": 2e-5, "lr_scheduler": "warmup_linear", "freeze_steps": 0, "warmup_steps": 0}]
      ]
  }

}
